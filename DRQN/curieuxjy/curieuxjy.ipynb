{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRQN 따라가보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* custom modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.wrapper import *\n",
    "\n",
    "from agents.DQN import Model as DQN_Agent\n",
    "\n",
    "from networks.network_bodies import SimpleBody, AtariBody\n",
    "\n",
    "from utils.ReplayMemory import ExperienceReplayMemory\n",
    "from utils.hyperparameters import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "* utils.hyperparameters에 정의되어 있는 Config 객체사용\n",
    "* device 지정\n",
    "* epsilon 정의\n",
    "* misc agent 변수\n",
    "* memory 변수\n",
    "* training 변수\n",
    "* Nstep controls\n",
    "* DRQN 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = config.device\n",
    "\n",
    "#epsilon variables\n",
    "config.epsilon_start = 1.0\n",
    "config.epsilon_final = 0.01\n",
    "config.epsilon_decay = 30000\n",
    "config.epsilon_by_frame = lambda frame_idx: config.epsilon_final + (config.epsilon_start - config.epsilon_final) * math.exp(-1. * frame_idx / config.epsilon_decay)\n",
    "\n",
    "#misc agent variables\n",
    "config.GAMMA=0.99\n",
    "config.LR=1e-4\n",
    "\n",
    "#memory\n",
    "config.TARGET_NET_UPDATE_FREQ = 1024\n",
    "config.EXP_REPLAY_SIZE = 10000\n",
    "config.BATCH_SIZE = 32\n",
    "\n",
    "#Learning control variables\n",
    "config.LEARN_START = 10000\n",
    "config.MAX_FRAMES=1500000\n",
    "\n",
    "#Nstep controls\n",
    "config.N_STEPS=1\n",
    "\n",
    "#DRQN Parameters\n",
    "config.SEQUENCE_LENGTH=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentExperienceReplayMemory:\n",
    "    def __init__(self, capacity, sequence_length=10):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.seq_length=sequence_length\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        finish = random.sample(range(0, len(self.memory)), batch_size)\n",
    "        begin = [x-self.seq_length for x in finish]\n",
    "        samp = []\n",
    "        for start, end in zip(begin, finish):\n",
    "            #correct for sampling near beginning\n",
    "            final = self.memory[max(start+1,0):end+1]\n",
    "            \n",
    "            #correct for sampling across episodes\n",
    "            for i in range(len(final)-2, -1, -1):\n",
    "                if final[i][3] is None:\n",
    "                    final = final[i+1:]\n",
    "                    break\n",
    "                    \n",
    "            #pad beginning to account for corrections\n",
    "            while(len(final)<self.seq_length):\n",
    "                final = [(np.zeros_like(self.memory[0][0]), 0, 0, np.zeros_like(self.memory[0][3]))] + final\n",
    "                            \n",
    "            samp+=final\n",
    "\n",
    "        #returns flattened version\n",
    "        return samp, None, None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions, gru_size=512, bidirectional=False, body=AtariBody):\n",
    "        super(DRQN, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.gru_size = gru_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if self.bidirectional else 1\n",
    "\n",
    "        self.body = body(input_shape, num_actions)\n",
    "        self.gru = nn.GRU(self.body.feature_size(), self.gru_size, num_layers=1, batch_first=True, bidirectional=bidirectional)\n",
    "        #self.fc1 = nn.Linear(self.body.feature_size(), self.gru_size)\n",
    "        self.fc2 = nn.Linear(self.gru_size, self.num_actions)\n",
    "        \n",
    "    def forward(self, x, hx=None):\n",
    "        batch_size = x.size(0)\n",
    "        sequence_length = x.size(1)\n",
    "        \n",
    "        x = x.view((-1,)+self.input_shape)\n",
    "        \n",
    "        #format outp for batch first gru\n",
    "        feats = self.body(x).view(batch_size, sequence_length, -1)\n",
    "        hidden = self.init_hidden(batch_size) if hx is None else hx\n",
    "        out, hidden = self.gru(feats, hidden)\n",
    "        x = self.fc2(out)\n",
    "\n",
    "        return x, hidden\n",
    "        #return x\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1*self.num_directions, batch_size, self.gru_size, device=device, dtype=torch.float)\n",
    "    \n",
    "    def sample_noise(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(DQN_Agent):\n",
    "    def __init__(self, static_policy=False, env=None, config=None):\n",
    "        self.sequence_length=config.SEQUENCE_LENGTH\n",
    "\n",
    "        super(Model, self).__init__(static_policy, env, config)\n",
    "\n",
    "        self.reset_hx()\n",
    "    \n",
    "    def declare_networks(self):\n",
    "        self.model = DRQN(self.num_feats, self.num_actions, body=AtariBody)\n",
    "        self.target_model = DRQN(self.num_feats, self.num_actions, body=AtariBody)\n",
    "\n",
    "    def declare_memory(self):\n",
    "        self.memory = RecurrentExperienceReplayMemory(self.experience_replay_size, self.sequence_length)\n",
    "        #self.memory = ExperienceReplayMemory(self.experience_replay_size)\n",
    "\n",
    "    def prep_minibatch(self):\n",
    "        transitions, indices, weights = self.memory.sample(self.batch_size)\n",
    "\n",
    "        batch_state, batch_action, batch_reward, batch_next_state = zip(*transitions)\n",
    "\n",
    "        shape = (self.batch_size,self.sequence_length)+self.num_feats\n",
    "\n",
    "        batch_state = torch.tensor(batch_state, device=self.device, dtype=torch.float).view(shape)\n",
    "        batch_action = torch.tensor(batch_action, device=self.device, dtype=torch.long).view(self.batch_size, self.sequence_length, -1)\n",
    "        batch_reward = torch.tensor(batch_reward, device=self.device, dtype=torch.float).view(self.batch_size, self.sequence_length)\n",
    "        #get set of next states for end of each sequence\n",
    "        batch_next_state = tuple([batch_next_state[i] for i in range(len(batch_next_state)) if (i+1)%(self.sequence_length)==0])\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch_next_state)), device=self.device, dtype=torch.uint8)\n",
    "        try: #sometimes all next states are false, especially with nstep returns\n",
    "            non_final_next_states = torch.tensor([s for s in batch_next_state if s is not None], device=self.device, dtype=torch.float).unsqueeze(dim=1)\n",
    "            non_final_next_states = torch.cat([batch_state[non_final_mask, 1:, :], non_final_next_states], dim=1)\n",
    "            empty_next_state_values = False\n",
    "        except:\n",
    "            empty_next_state_values = True\n",
    "\n",
    "        return batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights\n",
    "    \n",
    "    def compute_loss(self, batch_vars):\n",
    "        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights = batch_vars\n",
    "\n",
    "        #estimate\n",
    "        current_q_values, _ = self.model(batch_state)\n",
    "        current_q_values = current_q_values.gather(2, batch_action).squeeze()\n",
    "        \n",
    "        #target\n",
    "        with torch.no_grad():\n",
    "            max_next_q_values = torch.zeros((self.batch_size, self.sequence_length), device=self.device, dtype=torch.float)\n",
    "            if not empty_next_state_values:\n",
    "                max_next, _ = self.target_model(non_final_next_states)\n",
    "                max_next_q_values[non_final_mask] = max_next.max(dim=2)[0]\n",
    "            expected_q_values = batch_reward + ((self.gamma**self.nsteps)*max_next_q_values)\n",
    "\n",
    "        diff = (expected_q_values - current_q_values)\n",
    "        loss = self.huber(diff)\n",
    "        \n",
    "        #mask first half of losses\n",
    "        split = self.sequence_length // 2\n",
    "        mask = torch.zeros(self.sequence_length, device=self.device, dtype=torch.float)\n",
    "        mask[split:] = 1.0\n",
    "        mask = mask.view(1, -1)\n",
    "        loss *= mask\n",
    "        \n",
    "        loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def get_action(self, s, eps=0.1):\n",
    "        with torch.no_grad():\n",
    "            self.seq.pop(0)\n",
    "            self.seq.append(s)\n",
    "            if np.random.random() >= eps or self.static_policy or self.noisy:\n",
    "                X = torch.tensor([self.seq], device=self.device, dtype=torch.float) \n",
    "                self.model.sample_noise()\n",
    "                a, _ = self.model(X)\n",
    "                a = a[:, -1, :] #select last element of seq\n",
    "                a = a.max(1)[1]\n",
    "                return a.item()\n",
    "            else:\n",
    "                return np.random.randint(0, self.num_actions)\n",
    "\n",
    "    #def get_max_next_state_action(self, next_states, hx):\n",
    "    #    max_next, _ = self.target_model(next_states, hx)\n",
    "    #    return max_next.max(dim=1)[1].view(-1, 1)'''\n",
    "\n",
    "    def reset_hx(self):\n",
    "        #self.action_hx = self.model.init_hidden(1)\n",
    "        self.seq = [np.zeros(self.num_feats) for j in range(self.sequence_length)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses, sigma, elapsed_time):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s. time: %s' % (frame_idx, np.mean(rewards[-10:]), elapsed_time))\n",
    "    plt.plot(rewards)\n",
    "    if losses:\n",
    "        plt.subplot(132)\n",
    "        plt.title('loss')\n",
    "        plt.plot(losses)\n",
    "    if sigma:\n",
    "        plt.subplot(133)\n",
    "        plt.title('noisy param magnitude')\n",
    "        plt.plot(sigma)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "start=timer()\n",
    "\n",
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env    = make_atari(env_id)\n",
    "env    = wrap_deepmind(env, frame_stack=False)\n",
    "env    = wrap_pytorch(env)\n",
    "model = Model(env=env, config=config)\n",
    "\n",
    "episode_reward = 0\n",
    "\n",
    "observation = env.reset()\n",
    "for frame_idx in range(1, config.MAX_FRAMES + 1):\n",
    "    # rendering\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    display.display(plt.gcf())    \n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    #-----------\n",
    "    \n",
    "    epsilon = config.epsilon_by_frame(frame_idx)\n",
    "    \n",
    "    action = model.get_action(observation, epsilon)\n",
    "    prev_observation=observation\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    observation = None if done else observation\n",
    "\n",
    "    model.update(prev_observation, action, reward, observation, frame_idx)\n",
    "    episode_reward += reward\n",
    "\n",
    "    if done:\n",
    "        model.finish_nstep()\n",
    "        model.reset_hx()\n",
    "        observation = env.reset()\n",
    "        model.save_reward(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "        if np.mean(model.rewards[-10:]) > 19:\n",
    "            plot(frame_idx, model.rewards, model.losses, model.sigma_parameter_mag, timedelta(seconds=int(timer()-start)))\n",
    "            break\n",
    "\n",
    "    if frame_idx % 10000 == 0:\n",
    "        plot(frame_idx, model.rewards, model.losses, model.sigma_parameter_mag, timedelta(seconds=int(timer()-start)))\n",
    "\n",
    "model.save_w()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
