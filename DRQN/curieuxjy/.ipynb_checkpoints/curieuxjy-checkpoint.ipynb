{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRQN 따라가보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* custom modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.wrapper import *\n",
    "\n",
    "from agents.DQN import Model as DQN_Agent\n",
    "\n",
    "from networks.network_bodies import SimpleBody, AtariBody\n",
    "\n",
    "from utils.ReplayMemory import ExperienceReplayMemory\n",
    "from utils.hyperparameters import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "* utils.hyperparameters에 정의되어 있는 Config 객체사용\n",
    "* device 지정\n",
    "* epsilon 정의\n",
    "* misc agent 변수\n",
    "* memory 변수\n",
    "* training 변수\n",
    "* Nstep controls\n",
    "* DRQN 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = config.device\n",
    "\n",
    "#epsilon variables\n",
    "config.epsilon_start = 1.0\n",
    "config.epsilon_final = 0.01\n",
    "config.epsilon_decay = 30000\n",
    "config.epsilon_by_frame = lambda frame_idx: config.epsilon_final + (config.epsilon_start - config.epsilon_final) * math.exp(-1. * frame_idx / config.epsilon_decay)\n",
    "\n",
    "#misc agent variables\n",
    "config.GAMMA=0.99\n",
    "config.LR=1e-4\n",
    "\n",
    "#memory\n",
    "config.TARGET_NET_UPDATE_FREQ = 1024\n",
    "config.EXP_REPLAY_SIZE = 10000\n",
    "config.BATCH_SIZE = 32\n",
    "\n",
    "#Learning control variables\n",
    "config.LEARN_START = 10000\n",
    "config.MAX_FRAMES=1500000\n",
    "\n",
    "#Nstep controls\n",
    "config.N_STEPS=1\n",
    "\n",
    "#DRQN Parameters\n",
    "config.SEQUENCE_LENGTH=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "* RecurrentExperienceReplayMemory 클래스\n",
    "* push\n",
    "    * memory에 넣기\n",
    "* sample\n",
    "    * sampling near beginning\n",
    "    * sampling across episodes\n",
    "    * pad beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentExperienceReplayMemory:\n",
    "    def __init__(self, capacity, sequence_length=10):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.seq_length=sequence_length\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        finish = random.sample(range(0, len(self.memory)), batch_size)\n",
    "        begin = [x-self.seq_length for x in finish]\n",
    "        samp = []\n",
    "        for start, end in zip(begin, finish):\n",
    "            #correct for sampling near beginning\n",
    "            final = self.memory[max(start+1,0):end+1]\n",
    "            \n",
    "            #correct for sampling across episodes\n",
    "            for i in range(len(final)-2, -1, -1):\n",
    "                if final[i][3] is None:\n",
    "                    final = final[i+1:]\n",
    "                    break\n",
    "                    \n",
    "            #pad beginning to account for corrections\n",
    "            while(len(final)<self.seq_length):\n",
    "                final = [(np.zeros_like(self.memory[0][0]), 0, 0, np.zeros_like(self.memory[0][3]))] + final\n",
    "                            \n",
    "            samp+=final\n",
    "\n",
    "        #returns flattened version\n",
    "        return samp, None, None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "* DRQN 클래스\n",
    "* forward\n",
    "    * networks.network_bodies에 정의된 AtariBody NN 구조 사용 = body\n",
    "    * gru 사용\n",
    "* init_hidden\n",
    "    * gru의 hidden layer를 0으로 초기화\n",
    "* sample_noise\n",
    "    * 안씀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions, gru_size=512, bidirectional=False, body=AtariBody):\n",
    "        super(DRQN, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.gru_size = gru_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if self.bidirectional else 1\n",
    "\n",
    "        self.body = body(input_shape, num_actions)\n",
    "        self.gru = nn.GRU(self.body.feature_size(), self.gru_size, num_layers=1, batch_first=True, bidirectional=bidirectional)\n",
    "        #self.fc1 = nn.Linear(self.body.feature_size(), self.gru_size)\n",
    "        self.fc2 = nn.Linear(self.gru_size, self.num_actions)\n",
    "        \n",
    "    def forward(self, x, hx=None):\n",
    "        batch_size = x.size(0)\n",
    "        sequence_length = x.size(1)\n",
    "        \n",
    "        x = x.view((-1,)+self.input_shape)\n",
    "        \n",
    "        #format outp for batch first gru\n",
    "        feats = self.body(x).view(batch_size, sequence_length, -1)\n",
    "        hidden = self.init_hidden(batch_size) if hx is None else hx\n",
    "        out, hidden = self.gru(feats, hidden)\n",
    "        x = self.fc2(out)\n",
    "\n",
    "        return x, hidden\n",
    "        #return x\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1*self.num_directions, batch_size, self.gru_size, device=device, dtype=torch.float)\n",
    "    \n",
    "    def sample_noise(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "* Model 클래스\n",
    "* agents.DQN에서 Model객체를 DQN_Agent로 가져옴\n",
    "* declare_networks\n",
    "    * model/target 따로 위에서 정의한 DRGN 클래스로 만들어줌\n",
    "* declare_memory\n",
    "    * memory를 위에서 정의한 RecurrentExperienceReplayMemory 클래스로 만들어줌\n",
    "* prep_minibatch\n",
    "    * batch_state, batch_action, batch_reward, batch_next_state 만들어줌\n",
    "* compute_loss\n",
    "    * loss 구하기\n",
    "* get_action\n",
    "* reset_hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(DQN_Agent):\n",
    "    def __init__(self, static_policy=False, env=None, config=None):\n",
    "        self.sequence_length=config.SEQUENCE_LENGTH\n",
    "\n",
    "        super(Model, self).__init__(static_policy, env, config)\n",
    "\n",
    "        self.reset_hx()\n",
    "    \n",
    "    def declare_networks(self):\n",
    "        self.model = DRQN(self.num_feats, self.num_actions, body=AtariBody)\n",
    "        self.target_model = DRQN(self.num_feats, self.num_actions, body=AtariBody)\n",
    "\n",
    "    def declare_memory(self):\n",
    "        self.memory = RecurrentExperienceReplayMemory(self.experience_replay_size, self.sequence_length)\n",
    "        #self.memory = ExperienceReplayMemory(self.experience_replay_size)\n",
    "\n",
    "    def prep_minibatch(self):\n",
    "        transitions, indices, weights = self.memory.sample(self.batch_size)\n",
    "\n",
    "        batch_state, batch_action, batch_reward, batch_next_state = zip(*transitions)\n",
    "\n",
    "        shape = (self.batch_size,self.sequence_length)+self.num_feats\n",
    "\n",
    "        batch_state = torch.tensor(batch_state, device=self.device, dtype=torch.float).view(shape)\n",
    "        batch_action = torch.tensor(batch_action, device=self.device, dtype=torch.long).view(self.batch_size, self.sequence_length, -1)\n",
    "        batch_reward = torch.tensor(batch_reward, device=self.device, dtype=torch.float).view(self.batch_size, self.sequence_length)\n",
    "        #get set of next states for end of each sequence\n",
    "        batch_next_state = tuple([batch_next_state[i] for i in range(len(batch_next_state)) if (i+1)%(self.sequence_length)==0])\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch_next_state)), device=self.device, dtype=torch.uint8)\n",
    "        try: #sometimes all next states are false, especially with nstep returns\n",
    "            non_final_next_states = torch.tensor([s for s in batch_next_state if s is not None], device=self.device, dtype=torch.float).unsqueeze(dim=1)\n",
    "            non_final_next_states = torch.cat([batch_state[non_final_mask, 1:, :], non_final_next_states], dim=1)\n",
    "            empty_next_state_values = False\n",
    "        except:\n",
    "            empty_next_state_values = True\n",
    "\n",
    "        return batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights\n",
    "    \n",
    "    def compute_loss(self, batch_vars):\n",
    "        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights = batch_vars\n",
    "\n",
    "        #estimate\n",
    "        current_q_values, _ = self.model(batch_state)\n",
    "        current_q_values = current_q_values.gather(2, batch_action).squeeze()\n",
    "        \n",
    "        #target\n",
    "        with torch.no_grad():\n",
    "            max_next_q_values = torch.zeros((self.batch_size, self.sequence_length), device=self.device, dtype=torch.float)\n",
    "            if not empty_next_state_values:\n",
    "                max_next, _ = self.target_model(non_final_next_states)\n",
    "                max_next_q_values[non_final_mask] = max_next.max(dim=2)[0]\n",
    "            expected_q_values = batch_reward + ((self.gamma**self.nsteps)*max_next_q_values)\n",
    "\n",
    "        diff = (expected_q_values - current_q_values)\n",
    "        loss = self.huber(diff)\n",
    "        \n",
    "        #mask first half of losses\n",
    "        split = self.sequence_length // 2\n",
    "        mask = torch.zeros(self.sequence_length, device=self.device, dtype=torch.float)\n",
    "        mask[split:] = 1.0\n",
    "        mask = mask.view(1, -1)\n",
    "        loss *= mask\n",
    "        \n",
    "        loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def get_action(self, s, eps=0.1):\n",
    "        with torch.no_grad():\n",
    "            self.seq.pop(0)\n",
    "            self.seq.append(s)\n",
    "            if np.random.random() >= eps or self.static_policy or self.noisy:\n",
    "                X = torch.tensor([self.seq], device=self.device, dtype=torch.float) \n",
    "                self.model.sample_noise()\n",
    "                a, _ = self.model(X)\n",
    "                a = a[:, -1, :] #select last element of seq\n",
    "                a = a.max(1)[1]\n",
    "                return a.item()\n",
    "            else:\n",
    "                return np.random.randint(0, self.num_actions)\n",
    "\n",
    "    #def get_max_next_state_action(self, next_states, hx):\n",
    "    #    max_next, _ = self.target_model(next_states, hx)\n",
    "    #    return max_next.max(dim=1)[1].view(-1, 1)'''\n",
    "\n",
    "    def reset_hx(self):\n",
    "        #self.action_hx = self.model.init_hidden(1)\n",
    "        self.seq = [np.zeros(self.num_feats) for j in range(self.sequence_length)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "3개의 그래프 plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses, sigma, elapsed_time):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s. time: %s' % (frame_idx, np.mean(rewards[-10:]), elapsed_time))\n",
    "    plt.plot(rewards)\n",
    "    if losses:\n",
    "        plt.subplot(132)\n",
    "        plt.title('loss')\n",
    "        plt.plot(losses)\n",
    "    if sigma:\n",
    "        plt.subplot(133)\n",
    "        plt.title('noisy param magnitude')\n",
    "        plt.plot(sigma)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM4AAAD8CAYAAAA/rZtiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAPkUlEQVR4nO3df+xV9X3H8eeLX9pCRX5ahlLQYaOsG1ZiXYzO1bX+yFK0iR1uMWw1QxPdatIlQ81a04Wk66r+s2iD0cgWq5JZlSzWyUhT20SrYBFBRL8I6hcIKDrRya/vl/f+uOdLr/i9cHmfe7nnfnk9km/uPZ9zzj3vw/2+vufcw73vq4jAzI7OsE4XYNaNHByzBAfHLMHBMUtwcMwSHByzhLYFR9JlkjZI6pG0sF3bMesEteP/cSQNB14Dvgb0Ai8A10TEKy3fmFkHtOuIcx7QExFvRMQ+4GFgbpu2ZXbMjWjT404F3q6b7gW+0mhhSYc97E0ePYwThqtFpZk15+1d/e9GxKTB5rUrOIP9ln8iHJIWAAsAxp0ovvcnYw//gMoHZ5jEV8//ZG7/59nnDrvOOWedxYSTD19Tvb379vGrVS+m6jvebPnjmWw7f+bB6bGb3mHm4y90sKLB3fzU+282mteu4PQCp9VNnwpsrV8gIhYDiwGmjR0RZYLRjKN9fKlcWO0I6v5towv/mdv1GucFYKakGZJGAfOAZW3altkx15YjTkT0SboJ+G9gOHB/RKxrx7bMOqFdp2pExJPAk+16/HZbs+E1hg1rfEAeP3YsXzpzZsP5NrS1LTjdrq+/H/r7G87f39d3DKuxqvFbbswSHByzBJ+qNfD706Zx0pjRDeePHDnyGFZjVePgNHDSmNFMOPnkTpdhFeVTNbMEB8cswadqTXrvfz/gvV0fNJzfd5hL1zb0ODhNen/XLjb1bul0GVYRPlUzS3BwzBJ8qtakz37mRCYe5vJ0fxzg/Q92HcOKrJMcnCZNmTSJKZMG/TAgAHv27vUH2Y4jPlUzS3BwzBKOm1O1PXv3HtXy+/bvP6p19u7bf7QlHbeG7+tj5Ie7D06P2L2vg9XkHBfBORBx1K8/1r7e06Zq7POrNvH5VZs6XUYp6VM1SadJ+oWk9ZLWSfpOMX67pC2SVhc/V7SuXLNqKHPE6QO+GxEvSvocsErS8mLeXRHx46YfSWLYCL9N37pHOjgRsQ3YVtz/UNJ6ao0Ij9r46bP4ywdWZEsxa4u/nzix4byWXFWTNB04B/hNMXSTpDWS7pc0rhXbMKuS0sGRNAZ4FLg5InYB9wBnALOpHZHuaLDeAkkrJa3cuXNn2TLMjqlSwZE0klpoHoyInwFExPaI6I+IA8C91Bqwf0pELI6IORExZ8KECWXKMDvmylxVE3AfsD4i7qwbn1K32FXA2nx5ZtVU5qraBcC1wMuSVhdjtwLXSJpNrcn6ZuD6UhWaVVCZq2q/ZvBvJeja7p1mzfJ71cwSHByzBAfHLKESb/LctXUjP/+nb3a6DLOmVSI4fXt3s3PTy50uw6xpPlUzS3BwzBIcHLMEB8cswcExS3BwzBIcHLMEB8cswcExS3BwzBIcHLMEB8csodSbPCVtBj4E+oG+iJgjaTzwCDCd2kenvxUR75cr06xaWnHE+dOImB0Rc4rphcCKiJgJrCimzYaUdpyqzQWWFPeXAFe2YRtmHVU2OAE8LWmVpAXF2ClFe9yBNrmTS27DrHLKfpDtgojYKmkysFzSq82uWARtAcC4E32NwrpLqd/YiNha3O4AHqPWtXP7QFPC4nZHg3UPdvIcM2qwLlNm1VWmk+fo4us9kDQa+Dq1rp3LgPnFYvOBJ8oWaVY1ZU7VTgEeq3XCZQTw04h4StILwFJJ1wFvAVeXL9OsWsp08nwD+KNBxncCl5Qpyqzq/KrcLMHBMUtwcMwSHByzhEp08jQrq+cb57LvpM8cnJ76q1cZ++a7bdueg2NDwu7xY9g7fszB6f4TR7Z1ez5VM0twcMwSHByzBAfHLMHBMUtwcMwSHByzBAfHLMHBMUtwcMwSHByzhPR71SR9kVrHzgGnA98DTgb+FninGL81Ip5MV2hWQWU+Or0BmA0gaTiwhVqnm78B7oqIH7ekQrMKatWp2iXAxoh4s0WPZ1ZprQrOPOChuumbJK2RdL+kcS3ahllDw/oPoP39B384EG3dniLKbUDSKGArMCsitks6BXiXWnvcfwamRMS3B1mvvpPnud+/eGypOsxa7ean3l9V92UCn9CKI87lwIsRsR0gIrZHRH9EHADupdbd81PcydO6WSuCcw11p2kD7W8LV1Hr7mk2pJT9YqnPAl8Drq8b/pGk2dRO1TYfMs9sSCgVnIj4GJhwyNi1pSoy6wJ+54BZgoNjluDgmCU4OGYJDo5ZgoNjluDgmCU4OGYJDo5ZgoNjluDgmCU4OGYJDo5ZgoNjluDgmCU4OGYJDo5ZwhGDU7R42iFpbd3YeEnLJb1e3I6rm3eLpB5JGyRd2q7CzTqpmSPOA8Blh4wtBFZExExgRTGNpLOp9VibVaxzd9Hl02xIOWJwIuIZ4L1DhucCS4r7S4Ar68Yfjoi9EbEJ6KFBeyizbpZ9jXNKRGwDKG4nF+NTgbfrlustxj5F0gJJKyWt/Ghfe7sumrVaqy8ODNZZcNBUuCGhdbNscLYPNB4sbncU473AaXXLnUqtPa7ZkJINzjJgfnF/PvBE3fg8SSdImgHMBJ4vV6JZ9RyxIaGkh4CLgYmSeoHvAz8Elkq6DngLuBogItZJWgq8AvQBN0ZEf5tqN+uYIwYnIq5pMOuSBssvAhaVKcqs6vzOAbMEB8cswcExS3BwzBIcHLMEB8cswcExS3BwzBIcHLMEB8cswcExS3BwzBIcHLMEB8cswcExS3BwzBIcHLOEbCfPf5X0qqQ1kh6TdHIxPl3Sbkmri5+ftLN4s07JdvJcDvxBRPwh8BpwS928jRExu/i5oTVlmlVLqpNnRDwdEX3F5HPU2kCZHTda8Rrn28DP66ZnSPqtpF9KurDRSu7kad3siF1uDkfSbdTaQD1YDG0DpkXETknnAo9LmhURuw5dNyIWA4sBpo0d4eRYV0kfcSTNB/4c+KuICICi2frO4v4qYCNwZisKNauSVHAkXQb8I/CNiPi4bnzSwNd6SDqdWifPN1pRqFmVZDt53gKcACyXBPBccQXtIuAHkvqAfuCGiDj0K0LMul62k+d9DZZ9FHi0bFFmVed3DpglODhmCQ6OWYKDY5bg4JglODhmCQ6OWYKDY5bg4JglODhmCQ6OWYKDY5bg4JglODhmCQ6OWYKDY5bg4JglZDt53i5pS13Hzivq5t0iqUfSBkmXtqtws07KdvIEuKuuY+eTAJLOBuYBs4p17h5o3mE2lKQ6eR7GXODhok3UJqAHOK9EfWaVVOY1zk1F0/X7JY0rxqYCb9ct01uMfYo7eVo3ywbnHuAMYDa17p13FOMaZNlBUxERiyNiTkTMGTNqsNXMqivVAjcitg/cl3Qv8F/FZC9wWt2ipwJb09U1aZjE8OG/eykVQF9fX+MVzEpKBUfSlIjYVkxeBQxccVsG/FTSncDvUevk+XzpKo9g8oQJfOnMmQenP/r4Y55d/VK7N2vHsWwnz4slzab2x30zcD1ARKyTtBR4hVoz9hsjor89pZt1Tks7eRbLLwIWlSnKrOr8zgGzBAfHLMHBMUtwcMwSHByzBAfHLMHBMUtwcMwSHByzBAfHLMHBMUtwcMwSHByzhNTncarmg48+4pWNGw9O9/X5kwzWXkMiOLv37GHLnj2dLsOOIz5VM0vINiR8pK4Z4WZJq4vx6ZJ21837STuLN+uUZk7VHgD+Dfj3gYGI+IuB+5LuAD6oW35jRMxuVYFmVdTMR6efkTR9sHmSBHwL+GpryzKrtrKvcS4EtkfE63VjMyT9VtIvJV1Y8vHNKqnsVbVrgIfqprcB0yJip6RzgcclzYqIXYeuKGkBsABg3Im+RmHdJf0bK2kE8E3gkYGxomf0zuL+KmAjcOZg67uTp3WzMn/q/wx4NSJ6BwYkTRr4dgJJp1NrSPhGuRLNqqeZy9EPAc8CX5TUK+m6YtY8PnmaBnARsEbSS8B/AjdERLPfdGDWNbINCYmIvx5k7FHg0fJlmVWbX5WbJTg4ZgkOjlmCg2OW4OCYJTg4ZgkOjlmCg2OW4OCYJTg4ZgkOjlmCg2OW4OCYJTg4ZgmVaEg4ZvI0Lvy7H3S6DLNPeurahrMqEZxRo0/iC1+5vNNlmDXNp2pmCc18dPo0Sb+QtF7SOknfKcbHS1ou6fXidlzdOrdI6pG0QdKl7dwBs05o5ojTB3w3Is4CzgdulHQ2sBBYEREzgRXFNMW8ecAs4DLg7oEGHmZDxRGDExHbIuLF4v6HwHpgKjAXWFIstgS4srg/F3i4aBW1CegBzmt14WaddFSvcYpWuOcAvwFOiYhtUAsXMLlYbCrwdt1qvcWY2ZDRdHAkjaHWwebmwTpz1i86yFgM8ngLJK2UtHLnzp3NlmFWCU0FR9JIaqF5MCJ+VgxvlzSlmD8F2FGM9wKn1a1+KrD10Mes7+Q5YcKEbP1mHdHMVTUB9wHrI+LOulnLgPnF/fnAE3Xj8ySdIGkGtW6ez7euZLPOa+Y/QC8ArgVeHvgCKeBW4IfA0qKz51vA1QARsU7SUuAValfkbowIfymnDSnNdPL8NYO/bgG4pME6i4BFJeoyqzS/c8AswcExS3BwzBIcHLMEB8csQRGf+k/9Y1+E9A7wf8C7na6lhSYydPZnKO0LNL8/X4iISYPNqERwACStjIg5na6jVYbS/gylfYHW7I9P1cwSHByzhCoFZ3GnC2ixobQ/Q2lfoAX7U5nXOGbdpEpHHLOu0fHgSLqsaOrRI2lhp+vJkLRZ0suSVktaWYw1bGZSNZLul7RD0tq6sa5txtJgf26XtKV4jlZLuqJu3tHvT0R07AcYDmwETgdGAS8BZ3eypuR+bAYmHjL2I2BhcX8h8C+drvMw9V8EfBlYe6T6gbOL5+kEYEbx/A3v9D40sT+3A/8wyLKp/en0Eec8oCci3oiIfcDD1Jp9DAWNmplUTkQ8A7x3yHDXNmNpsD+NpPan08EZKo09Anha0ipJC4qxRs1MusVQbMZyk6Q1xancwKlnan86HZymGnt0gQsi4svA5dT6zl3U6YLaqFufs3uAM4DZwDbgjmI8tT+dDk5TjT2qLiK2Frc7gMeoHeobNTPpFqWasVRNRGyPiP6IOADcy+9Ox1L70+ngvADMlDRD0ihqHUCXdbimoyJptKTPDdwHvg6spXEzk24xpJqxDPwRKFxF7TmC7P5U4ArIFcBr1K5m3NbpehL1n07tqsxLwLqBfQAmUGsN/HpxO77TtR5mHx6idvqyn9pf4OsOVz9wW/F8bQAu73T9Te7PfwAvA2uKsEwpsz9+54BZQqdP1cy6koNjluDgmCU4OGYJDo5ZgoNjluDgmCU4OGYJ/w+U/aEx2vpXVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "start=timer()\n",
    "\n",
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env    = make_atari(env_id)\n",
    "env    = wrap_deepmind(env, frame_stack=False)\n",
    "env    = wrap_pytorch(env)\n",
    "model = Model(env=env, config=config)\n",
    "\n",
    "episode_reward = 0\n",
    "\n",
    "observation = env.reset()\n",
    "for frame_idx in range(1, config.MAX_FRAMES + 1):\n",
    "    # rendering\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    display.display(plt.gcf())    \n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    #-----------\n",
    "    \n",
    "    epsilon = config.epsilon_by_frame(frame_idx)\n",
    "    \n",
    "    action = model.get_action(observation, epsilon)\n",
    "    prev_observation=observation\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    observation = None if done else observation\n",
    "\n",
    "    model.update(prev_observation, action, reward, observation, frame_idx)\n",
    "    episode_reward += reward\n",
    "\n",
    "    if done:\n",
    "        model.finish_nstep()\n",
    "        model.reset_hx()\n",
    "        observation = env.reset()\n",
    "        model.save_reward(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "        if np.mean(model.rewards[-10:]) > 19:\n",
    "            plot(frame_idx, model.rewards, model.losses, model.sigma_parameter_mag, timedelta(seconds=int(timer()-start)))\n",
    "            break\n",
    "\n",
    "    if frame_idx % 10000 == 0:\n",
    "        plot(frame_idx, model.rewards, model.losses, model.sigma_parameter_mag, timedelta(seconds=int(timer()-start)))\n",
    "\n",
    "model.save_w()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
